## **4. Problem Statements & Example Projects**

**Problem Statement One:** Build a Tool That Should Exist — Create the AI-native app or workflow you wish someone had already built. Eliminate busywork. Make hard things effortless.

**Example Projects:**

- Contract Lifecycle Autopilot - Extracts all obligations from contracts and tracks deadlines with automatic reminders

- Product Changelog Publisher - Turns release notes into customer-facing announcements across multiple channels

- Bug Report Enricher - Automatically adds system logs, user history, and reproduction steps to support tickets

**Problem Statement Two:** Break the Barriers — Expert knowledge, essential tools, AI's benefits — take something powerful that's locked behind expertise, cost, language, or infrastructure and put it in everyone's hands.

**Example Projects:**

- Crop Doctor — Combines image analysis, weather data, and soil reports to diagnose plant diseases and recommend organic treatment protocols.

- Accessibility Auditor — Evaluates websites, documents, and physical spaces against accessibility standards and generates remediation plans.

- Open Source Hardware Guide — Helps makers navigate component selection, PCB design, and manufacturing for hardware projects.

**Problem Statement Three:** Amplify Human Judgment — Build AI that makes researchers, professionals, and decision-makers dramatically more capable — without taking them out of the loop. The best AI doesn't replace human expertise. It sharpens it.

**Example Projects:**

- Brand Safety Monitor - Reviews ad placements and content adjacencies, flagging reputation risks for marketing teams

- Discovery Anomaly Detector - Flags documents in discovery that should exist but are missing based on references in other documents

- Grading Calibration Partner — Highlights scoring inconsistencies in instructor assessments, supporting but not overriding professional judgment on standards.

## **5. Anthropic-Provided Resources**

**Quickstarts**

[Claude Code Quickstart](https://code.claude.com/docs/en/quickstart)

[Claude API Quickstart](https://platform.claude.com/docs/en/get-started)

[Claude Models Overview](https://platform.claude.com/docs/en/about-claude/models/overview)

**Docs**

[Claude Code Docs](https://code.claude.com/docs)

[Claude API Docs](https://platform.claude.com/docs/en/home)

[MCP Docs](https://modelcontextprotocol.io/docs/getting-started/intro)

[Agent Skills Docs](https://agentskills.io/home)

**Blogs**

[Claude Code Best Practices](https://code.claude.com/docs/en/best-practices)

[Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)

[Building Agents with the Claude Agent SDK](https://claude.com/blog/building-agents-with-the-claude-agent-sdk)

[Building multi-agent systems: when and how to use them](https://claude.com/blog/building-multi-agent-systems-when-and-how-to-use-them)

[Best practices for prompt engineering](https://claude.com/blog/best-practices-for-prompt-engineering)

[Effective Context Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)

[Extending Claude’s capabilities with skills and MCP servers](https://claude.com/blog/extending-claude-capabilities-with-skills-mcp-servers)

[Skills explained: How Skills compares to prompts, Projects, MCP, and subagents](https://claude.com/blog/skills-explained)

[Building agents with Skills: Equipping agents for specialized work](https://claude.com/blog/building-agents-with-skills-equipping-agents-for-specialized-work)

[Claude Code power user customization: How to configure hooks](https://claude.com/blog/how-to-configure-hooks)

**Courses**

[Claude Code in Action](https://anthropic.skilljar.com/claude-code-in-action)

[Agent Skills with Anthropic](https://www.deeplearning.ai/short-courses/agent-skills-with-anthropic/)

[Claude Code Courses GitHub](https://github.com/anthropics/courses) (note - this might no longer be maintained?)

**Other**

[Claude Quickstarts](https://github.com/anthropics/claude-quickstarts) - A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API 

[A Complete Guide to Building Skills for Claude (eBook)](https://claude.com/blog/complete-guide-to-building-skills-for-claude)

[Claude Cookbooks](https://platform.claude.com/cookbook/) and [Cookbooks Github Repo](https://github.com/anthropics/claude-cookbooks)

[Agent Skills Github Repo](https://github.com/anthropics/skills)

## **6. Judging**

Judging for the Anthropic Virtual Hackathon happens in **two stages**:

### **Stage 1 – Asynchronous Judging**

- **Date:** Feb 16th - Feb 17th

- **How it works:**

  - Judges review your **submitted projects asynchronously** via the judging platform.

  - Each team will have uploaded:

    1. A **short demo video** (3 minute maximum)

    2. Open Source Project repository / code

    3. Written summary (100–200 words)

  - Judges independently evaluate projects using **standardized criteria**.\
    **Judging Criteria:**

    1. **Impact (25%) -** What's the real-world potential here? Who benefits, and how much does it matter? Could this actually become something people use? Does it fit into one of the problem statements listed above?

    2. **Opus 4.6 Use (25%) -** How creatively did this team use Opus 4.6? Did they go beyond a basic integration? Did they surface capabilities that surprised even us?

    3. **Depth & Execution (20%) —** Did the team push past their first idea? Is the engineering sound and thoughtfully refined? Does this feel like something that was wrestled with — real craft, not just a quick hack?

    4. **Demo (30%) *—*** Is this a working, impressive demo? Does it hold up live? Is it genuinely cool to watch?

  - After evaluation, the **team aggregates scores** to determine the **Top 6 projects** for the final round
